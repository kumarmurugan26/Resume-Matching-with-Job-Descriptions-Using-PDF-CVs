# -*- coding: utf-8 -*-
"""resume matching JD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cGuxH4GeoGPLz4wucozcZcVt6cEDblu0
"""

!pip install PyPDF2
!pip install transformers
!pip install datasets
!pip install fitz
!pip install PyMuPDF

import os
import PyPDF2
import fitz
import re
import torch
from transformers import DistilBertTokenizer, DistilBertModel
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from datasets import load_dataset

import zipfile

with zipfile.ZipFile('archive (2).zip', 'r') as zip_ref:
    zip_ref.extractall('/content/')

def get_pdfs(data_folder_path):

  pdfs = []
  for root, dirs, files in os.walk(data_folder_path):
    for file in files:
      if file.endswith(".pdf"):
        pdfs.append(os.path.join(root, file))
  return pdfs

# Get the path to the data folder.
data_folder_path = "/content/data"

# Get a list of all PDF files in the data folder.
pdfs = get_pdfs(data_folder_path)

# Print the list of PDF files.
for pdf in pdfs:
  print(pdf)

len(pdfs)

# List of resume PDF file paths
resume_files = pdfs

def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        pdf_document = fitz.open(pdf_path)
        for page_num in range(len(pdf_document)):
            page = pdf_document[page_num]
            text += page.get_text()
    except Exception as e:
        print(f"Error reading PDF file: {e}")
    return text

# Function to extract Category, Skills, and Education
def extract_details(text):
    # Extract Category (Job Role) based on the first line
    category_match = re.search(r'^([^\n]+)', text)
    if category_match:
        category = category_match.group(1).strip()
    else:
        category = None

    education_pattern = r'Education((?:(?!Skills|Experience|Accomplishments|Work History|ProfessionalExperience|Languages|Additional Information|Highlights|Interests).)+)'
    education_match = re.search(education_pattern, text, re.DOTALL | re.IGNORECASE)
    if education_match:
        education = education_match.group(1).strip()
    else:
        education = None

    # Extract Skills
    skills_pattern = r'Skills((?:(?!Education|Education and Training|Experience|Accomplishments|Work History|ProfessionalExperience|Languages|Additional Information|Highlights|Interests).)+)'
    skills_match = re.search(skills_pattern, text, re.DOTALL | re.IGNORECASE)
    if skills_match:
        skills = skills_match.group(1).strip()
    else:
        skills = None
    return {
        "Category": category,
        "Skills": skills,
        "Education": education,
        "File Name": resume_file
    }
# Process each resume and extract information
resume_data = []
for resume_file in resume_files:
    resume_text = extract_text_from_pdf(resume_file)
    extracted_info = extract_details(resume_text)
    if extracted_info:
        extracted_info["File Name"] = resume_file  # Add file name to the extracted info
        resume_data.append(extracted_info)

# Print the extracted information along with file names
for i, data in enumerate(resume_data, start=1):
    print(f"Resume {i} Data:")
    print(f"File Name: {data['File Name']}")
    print(f"Category: {data['Category']}")
    print(f"Skills:\n{data['Skills']}")
    print(f"Education:\n{data['Education']}")
    print("\n")

print(resume_data[1])

from transformers import DistilBertTokenizer, DistilBertModel

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

# Initialize an empty list to store the embeddings
resume_embeddings = []

for resume in resume_data:
    # Combine all relevant text fields
    text = f"{resume['Category']}\n{resume['Skills']}\n{resume['Education']}"

    # Tokenize and embed the text using DistilBERT
    tokens = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        embeddings = model(**tokens).last_hidden_state.mean(dim=1)

    # Now we have the embeddings for this resume, which we can use for similarity calculations
    resume_embeddings.append(embeddings)

resume_embeddings

# Convert the list of PyTorch tensors into a numpy array for easier computation
resume_embeddings_np = np.array([embeddings.numpy().flatten() for embeddings in resume_embeddings])

resume_embeddings_np

from IPython.core.display import display, HTML
display(HTML("<style>.container { width:100% !important; }</style>"))

dataset = load_dataset("jacob-hugging-face/job-descriptions")
job_descriptions = dataset["train"]["job_description"][:10]
company_names = dataset["train"]["company_name"][:10]

# Initialize DistilBERT tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

# Tokenize and embed job descriptions
job_description_embeddings = []
for job_desc in job_descriptions:
    tokens = tokenizer(job_desc, return_tensors='pt', padding=True, truncation=True)
    with torch.no_grad():
        embeddings = model(**tokens)
    # Take the mean of embeddings across tokens to get a single vector for the entire description
    job_desc_embedding = torch.mean(embeddings.last_hidden_state, dim=1)
    job_description_embeddings.append(job_desc_embedding)

# Convert the list of embeddings to a NumPy array for efficient cosine similarity calculation
job_description_embeddings = torch.cat(job_description_embeddings).numpy()

# Print company names and job descriptions
for company_name, job_desc in zip(company_names, job_descriptions):
    print(f"Company Name: {company_name}")
    print(f"Job Description: {job_desc}")
    print("-" * 50)

job_description_embeddings

from sklearn.metrics.pairwise import cosine_similarity

# Initialize a list to store the top 5 resume indices for each job description
top_resume_indices = []

# Iterate through each job description and its embedding
for job_desc_embedding, job_desc_text in zip(job_description_embeddings, job_descriptions):
    # Calculate cosine similarities between the current job description and all resumes
    similarities = cosine_similarity([job_desc_embedding], resume_embeddings_np)

    # Get the indices of the top 5 most similar resumes
    top_indices = similarities.argsort()[0][::-1][:5]
    top_resume_indices.append(top_indices)

# Now, top_resume_indices contains the top 5 resume indices for each job description

# Display the results
for i, top_indices in enumerate(top_resume_indices):
    print(f"Company Name: {company_names[i]}")
    print(f"Job Description: {job_descriptions[i]}")


    # Calculate cosine similarities for the current job description
    similarities = cosine_similarity([job_description_embeddings[i]], resume_embeddings_np)

    for j, idx in enumerate(top_indices):
        print(f"Top Resume {j + 1}: Similarity Score = {similarities[0][idx]}")
        resume_info = resume_data[idx]  # Assuming resume_data contains the resume content
        print(f"File Name: {resume_info.get('File Name', 'Not Available')}")
        # print(resume_info)  # Print the resume content
        print("\n")
    print("-" * 50)